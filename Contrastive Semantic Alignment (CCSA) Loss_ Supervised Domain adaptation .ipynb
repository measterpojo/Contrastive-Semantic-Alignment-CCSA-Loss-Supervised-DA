{"cells":[{"cell_type":"markdown","metadata":{"id":"d1kDjdWS-PRb"},"source":["**Why use CCSA?**\n","\n","*   **Semantic Alignment:** It minimizes the distance between samples of the same class across domains while maximizing the separation between samples of different classes. This ensures that features are well-aligned semantically, which is crucial for DA\n","*   **Unified Framework:** It combines classification loss with contrastive loss, providing a comprehensive approach to supervised domain adaptation and generalization\n","*   **Fast Adaptation**: The method demonstrates a high \"speed\" of adaptation, meaning it can quickly achieve strong performance even with limited labeled target data\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Zbb3LOy1ANDh"},"source":["**Why is CCSA good for supervised domain adaptation?**\n","\n","*   **Leveraging Supervision:** Unlike unsupervised approaches, supervised domain adaptation benefits from labeled data in both the source and target domains. CCSA Loss explicitly uses these labels to enforce semantic alignment, which leads to better classification performance.\n","*   **Class Discrimination:** By maximizing inter-class variance, CCSA Loss ensures that different classes are well-separated in the feature space. This helps the model maintain high discriminative power across domains, making it robust to variations.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rRk0WDlPHyi8"},"source":["**Imports**"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3341,"status":"ok","timestamp":1744249241797,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"kBI6_YNbHgtC"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","import torchvision.models as models\n","from torchvision import datasets, transforms\n","\n","from torch.utils.data import WeightedRandomSampler\n","from torch.utils.data import DataLoader\n","\n","from collections import Counter\n","from torch.utils.data import random_split"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1744249241802,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"BcyD_jY3CCpb","outputId":"f58b5f2d-0eb2-4d57-dbdb-af26aec32b98"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'cuda'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"source":["# Setup device-agnostic code\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{"id":"h94FFJ-eB6e3"},"source":["**Data Processing**"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1231,"status":"ok","timestamp":1744249243033,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"fmu-ot6uB5ub","outputId":"d55aa4f3-c4cb-429e-e132-2e240f0aee94"},"outputs":[{"output_type":"stream","name":"stdout","text":["Path to dataset files: /kaggle/input/office31\n"]}],"source":["import kagglehub\n","\n","# Download latest version\n","path = kagglehub.dataset_download(\"xixuhu/office31\")\n","\n","print(\"Path to dataset files:\", path)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1744249243038,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"-Gqe2liYCQOK"},"outputs":[],"source":["import os\n","def walk_through_dir(dir_path):\n","\n","  for dirpath, dirnames, filenames in os.walk(dir_path):\n","    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n","\n","\n","# uncomment to find the path of the 3 datasets\n","# walk_through_dir(path)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1744249243042,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"AvwGFnisCfsi"},"outputs":[],"source":["amazon_path = '/root/.cache/kagglehub/datasets/xixuhu/office31/versions/1/Office-31/amazon'\n","dslr_path = '/root/.cache/kagglehub/datasets/xixuhu/office31/versions/1/Office-31/dslr'\n","webcam_path = '/root/.cache/kagglehub/datasets/xixuhu/office31/versions/1/Office-31/webcam'"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":56,"status":"ok","timestamp":1744249243100,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"Ar2LGluCDeSq"},"outputs":[],"source":["simple_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","    ])\n","\n","\n","target_transform = transforms.Compose([\n","    transforms.Resize((224, 224)),\n","    transforms.RandomHorizontalFlip(p=0.5),\n","    transforms.RandomRotation(15),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","amazon_data = datasets.ImageFolder(amazon_path, transform=simple_transform)\n","# dslr_data = datasets.ImageFolder(dslr_path, transform=simple_transform)\n","webcam_data = datasets.ImageFolder(webcam_path, transform=target_transform)\n"]},{"cell_type":"code","source":["# split the target domain (amazon) to make val dataset\n","val_size = int(len(amazon_data) * 0.2)\n","train_size = len(amazon_data) - val_size\n","amazon_dataset, amazon_val_dataset = random_split(amazon_data, [train_size, val_size])"],"metadata":{"id":"GCKOUM4w3xkO","executionInfo":{"status":"ok","timestamp":1744249243102,"user_tz":300,"elapsed":5,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["batch_size = 32\n","\n","# Calculate weights for oversampling\n","target_weights = [1.0] * len(webcam_data)\n","target_sampler = WeightedRandomSampler(target_weights, num_samples=len(amazon_dataset), replacement=True)"],"metadata":{"id":"KbzCEgIpGMHD","executionInfo":{"status":"ok","timestamp":1744249243105,"user_tz":300,"elapsed":3,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# DataLoaders\n","batch_size = 32\n","\n","amazon_dataloader = DataLoader(amazon_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","# dslr_dataloader = DataLoader(dslr_data, batch_size=batch_size, shuffle=True, drop_last=True)\n","webcam_dataloader = DataLoader(webcam_data, batch_size=batch_size, sampler=target_sampler,  drop_last=True)\n","\n","amazon_val_dataloader = DataLoader(amazon_val_dataset, batch_size=batch_size, shuffle=True)\n","\n"],"metadata":{"id":"DoI-mtSC3uyl","executionInfo":{"status":"ok","timestamp":1744249243108,"user_tz":300,"elapsed":2,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["len(amazon_dataloader), len(webcam_dataloader), len(amazon_val_dataloader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vYsNcWDm9yRJ","executionInfo":{"status":"ok","timestamp":1744249243116,"user_tz":300,"elapsed":7,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"8528ef1c-2f89-4e02-d938-bd3875b89382"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70, 70, 18)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["def count_class_instances(dataset):\n","    \"\"\"\n","    Count the number of instances for each class in the dataset.\n","\n","    Args:\n","        dataset (Dataset): A PyTorch Dataset object where each item returns (data, label).\n","\n","    Returns:\n","        dict: A dictionary with class labels as keys and their counts as values.\n","    \"\"\"\n","    # Count occurrences of each class label\n","    class_counts = Counter([label for _, label in dataset])\n","\n","    # Print the counts\n","    print(\"Class distribution:\")\n","    for cls, count in class_counts.items():\n","        print(f\"Class {cls}: {count} instances\")\n","\n","    return class_counts\n","\n","# amazon_class_counts = count_class_instances(amazon_data)\n","# dslr_class_counts = count_class_instances(dslr_data)\n","# webcam_class_counts = count_class_instances(webcam_data)\n"],"metadata":{"id":"WuFvvdyz2LkE","executionInfo":{"status":"ok","timestamp":1744249243118,"user_tz":300,"elapsed":1,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":91,"status":"ok","timestamp":1744249243210,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"7rBIPhFlD9i4","outputId":"264274b7-9ba0-4d43-f93c-037cc78c6c48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Batch of images shape: torch.Size([32, 3, 224, 224])\n","Batch of labels: tensor([25, 16,  1,  3,  3, 25,  8, 17, 23, 21, 21, 18, 28,  9, 25, 25, 27,  6,\n","         0,  8, 18,  2, 18, 10, 24, 15, 10, 26, 28,  8,  2,  6])\n"]}],"source":["for images, labels in amazon_dataloader:\n","    print(\"Batch of images shape:\", images.shape)  # E.g., [4, 3, 224, 224] for 4 images, 3 color channels, 224x224 size\n","    print(\"Batch of labels:\", labels)             # E.g., tensor([0, 1, 0, 2]) depending on classes\n","    break"]},{"cell_type":"markdown","metadata":{"id":"iPle-OXw7xJS"},"source":["**Models**"]},{"cell_type":"markdown","metadata":{"id":"j6fmhLHU74HM"},"source":["Feature extractor\n","\n","\n","---\n","\n","\n","The backbone of your model will extract meaningful features from input images\n","\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1744249243213,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"Mq9rtM3v8CHc"},"outputs":[],"source":["class FeatureExtractor(nn.Module):\n","  def __init__(self):\n","    super().__init__()\n","    backbone = models.resnet18(pretrained=True)\n","    self.feature_extractor = nn.Sequential(*list(backbone.children())[:-1])\n","\n","    # Freeze early layers\n","    for param in list(self.feature_extractor.parameters())[:-3]:  # Freeze all but the last 3 layers\n","        param.requires_grad = False\n","\n","  def forward(self, x):\n","    return self.feature_extractor(x)"]},{"cell_type":"markdown","metadata":{"id":"hlvaOB3R9TLp"},"source":["Embedding Layer\n","\n","\n","---\n","\n","After extracting features, an embedding layer maps the features into a lower-dimensional space where domain alignment takes place. This layer can be a simple fully connected layer\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1744249243215,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"RpIquzK2A0zB"},"outputs":[],"source":["class EmbeddingLayer(nn.Module):\n","    def __init__(self, input_dim, embedding_dim):\n","        super(EmbeddingLayer, self).__init__()\n","        self.fc = nn.Linear(input_dim, embedding_dim)\n","        self.dropout = nn.Dropout(0.5)\n","\n","    def forward(self, x):\n","        x = self.fc(x)\n","        x = F.relu(x)\n","        x = self.dropout(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"KIhAU0TLBKkL"},"source":["Combined Architecture\n","\n","---\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1744249243217,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"nipwPOzsBLSn"},"outputs":[],"source":["class DomainAdaptationModel(nn.Module):\n","    def __init__(self, embedding_dim=128, num_classes = 31):\n","        super(DomainAdaptationModel, self).__init__()\n","        self.feature_extractor = FeatureExtractor()\n","        self.embedding_layer = EmbeddingLayer(input_dim=512, embedding_dim=embedding_dim)  # ResNet18 output size is 512\n","        self.classifier = nn.Linear(embedding_dim, num_classes)\n","\n","    def forward(self, x):\n","        features = self.feature_extractor(x)\n","        features = features.view(features.size(0), -1)\n","        embeddings = self.embedding_layer(features)\n","        embeddings = self.classifier(embeddings)\n","        return embeddings\n"]},{"cell_type":"markdown","metadata":{"id":"_fdEhgSoKwV4"},"source":["**CCSA Loss**"]},{"cell_type":"markdown","metadata":{"id":"5EnIE4t-3ILK"},"source":["CCSA Loss works by minimizing the distance between samples of the same class from different domains while maximizing the distance between samples of different classes. This involves calculating pairwise distances for embeddings. Focuses on aligning features across domains by minimizing intra-class variance and maximizing inter-class variance. It's ideal for supervised domain adaptation tasks."]},{"cell_type":"code","execution_count":40,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1744250438031,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"GLE7UScxLCDx"},"outputs":[],"source":["class CCSALoss(nn.Module):\n","    def __init__(self, margin=2):\n","        super(CCSALoss, self).__init__()\n","        self.margin = margin\n","\n","    def forward(self, embeddings, labels, domains):\n","        # Calculate pairwise distances\n","        pairwise_dist = torch.cdist(embeddings, embeddings, p=2)  # Euclidean distance\n","\n","        # Masks for same labels and different domains\n","        same_label_mask = labels.unsqueeze(1) == labels.unsqueeze(0)\n","        different_domain_mask = domains.unsqueeze(1) != domains.unsqueeze(0)\n","\n","        # Positive pairs: Same class, different domain\n","        positive_mask = same_label_mask & different_domain_mask\n","        positive_loss = pairwise_dist[positive_mask].sum()\n","        num_positive_pairs = positive_mask.sum().item()\n","        if num_positive_pairs > 0:\n","            positive_loss /= num_positive_pairs  # Normalize positive loss\n","\n","        # Negative pairs: Different class\n","        negative_mask = ~same_label_mask\n","        negative_loss = torch.clamp(self.margin - pairwise_dist[negative_mask], min=0).sum()\n","        num_negative_pairs = negative_mask.sum().item()\n","        if num_negative_pairs > 0:\n","            negative_loss /= num_negative_pairs  # Normalize negative loss\n","\n","        # Combine losses\n","        loss = positive_loss + negative_loss\n","        return loss\n","\n"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":189,"status":"ok","timestamp":1744250439326,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"R5qkvGjcF7J8"},"outputs":[],"source":["model = DomainAdaptationModel()\n","criterion = CCSALoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-4)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"]},{"cell_type":"markdown","metadata":{"id":"FyJxhTHqERqo"},"source":["**Training Loop**"]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1744250439819,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"},"user_tz":300},"id":"jWuAn726ETGk"},"outputs":[],"source":["def train_loop(\n","    model,\n","    ccsa_loss,\n","    optimizer,\n","    source_dataloader,\n","    target_dataloader,\n","    num_epochs,\n","    device\n","):\n","    \"\"\"\n","    Train a domain adaptation model with two datasets (source and target).\n","\n","    Args:\n","        model (nn.Module): The domain adaptation model.\n","        ccsa_loss (nn.Module): The CCSA loss function.\n","        optimizer (torch.optim.Optimizer): Optimizer for the model.\n","        source_dataloader (DataLoader): Dataloader for the source domain.\n","        target_dataloader (DataLoader): Dataloader for the target domain.\n","        num_epochs (int): Number of epochs to train.\n","        device (torch.device): The device (CPU/GPU) for training.\n","    \"\"\"\n","    model.to(device)\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_loss = 0.0\n","\n","        # Zip through source and target dataloaders\n","        for source_batch, target_batch in zip(source_dataloader, target_dataloader):\n","            # Prepare source data\n","            source_data, source_labels = source_batch\n","            source_domains = torch.zeros(source_data.size(0), dtype=torch.long).to(device)  # 0 indicates source domain\n","\n","            # Prepare target data\n","            target_data, target_labels = target_batch\n","            target_domains = torch.ones(target_data.size(0), dtype=torch.long).to(device)  # 1 indicates target domain\n","\n","            # Combine source and target data\n","            combined_data = torch.cat([source_data, target_data], dim=0).to(device)\n","            combined_labels = torch.cat([source_labels, target_labels], dim=0).to(device)\n","            combined_domains = torch.cat([source_domains, target_domains], dim=0).to(device)\n","\n","            # Forward pass and compute embeddings\n","            embeddings = model(combined_data)\n","\n","            # Compute CCSA Loss\n","            loss = ccsa_loss(embeddings, combined_labels, combined_domains)\n","\n","            # Backpropagation\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","        # Learning rate adjustment after the epoch\n","        scheduler.step()\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(source_dataloader)}\")\n"]},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Mr9XzFvGD4J","executionInfo":{"status":"ok","timestamp":1744250719779,"user_tz":300,"elapsed":278199,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}},"outputId":"812f196c-7517-4063-a37b-5b9023c9ed52"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10, Loss: 1.981586091859\n","Epoch 2/10, Loss: 1.830453886304583\n","Epoch 3/10, Loss: 1.8580328992434911\n","Epoch 4/10, Loss: 1.8459636688232421\n","Epoch 5/10, Loss: 1.8349796022687639\n","Epoch 6/10, Loss: 1.8140858394759043\n","Epoch 7/10, Loss: 1.786916880948203\n","Epoch 8/10, Loss: 1.7980996659823827\n","Epoch 9/10, Loss: 1.7911572643688747\n","Epoch 10/10, Loss: 1.7589624319757735\n"]}],"source":["train_loop(\n","    model,\n","    criterion,\n","    optimizer,\n","    source_dataloader=webcam_dataloader,\n","    target_dataloader=amazon_dataloader,\n","    num_epochs=10,\n","    device=device\n",")"]},{"cell_type":"markdown","source":["**Testing loop**"],"metadata":{"id":"rIEzlRFV5z1Z"}},{"cell_type":"code","source":["def test_loop(model, dataloader,criterion, device):\n","  model.eval()\n","  total_loss = 0.0\n","  correct = 0\n","  total = 0\n","\n","  with torch.no_grad():\n","    for batch in dataloader:\n","      data, labels = batch\n","      data, labels = data.to(device), labels.to(device)\n","\n","      #forward pass\n","      outputs = model(data)\n","      loss = criterion(outputs, labels)\n","\n","      # accumate loss\n","      total_loss += loss.item()\n","\n","      # calculate accuracy\n","      _, predicted = torch.max(outputs, dim=1)\n","      # print('Predicted:', predicted, 'True label:', labels)\n","      correct += (predicted == labels).sum().item()\n","      total += labels.size(0)\n","\n","\n","  # compute average loss and accuracy\n","  avg_loss = total_loss / len(dataloader)\n","  accuracy = correct / total * 100\n","\n","  print(f\"Validation Loss: {avg_loss:.4f}, Validation Accuracy: {accuracy:.2f}%\")\n","  return avg_loss, accuracy"],"metadata":{"id":"jlQXJ3WK51_w","executionInfo":{"status":"ok","timestamp":1744250719783,"user_tz":300,"elapsed":21,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# # Assuming `amazon_val_dataloader` and necessary components are defined\n","# avg_loss, accuracy = test_loop(\n","#     model=model,\n","#     dataloader=amazon_val_dataloader,\n","#     criterion=torch.nn.CrossEntropyLoss(),\n","#     device=device\n","# )\n"],"metadata":{"id":"71Kk4H9I8b1v","executionInfo":{"status":"ok","timestamp":1744250732018,"user_tz":300,"elapsed":1,"user":{"displayName":"Peter Perez","userId":"12102864046237915346"}}},"execution_count":46,"outputs":[]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyOaJ6uXjd1ZRh5B0hmKxq5j"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}